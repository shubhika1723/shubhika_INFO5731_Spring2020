{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhika1723/shubhika_INFO5731_Spring2020/blob/main/shubhika_5731_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f352d12",
      "metadata": {
        "id": "3f352d12",
        "outputId": "7d70b3de-ffaa-45fe-94ce-192b028c176c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3137, 4)\n"
          ]
        }
      ],
      "source": [
        "# merging the dataset\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "shubhika_list_df = []\n",
        "#shubhika_df = pd.DataFrame()\n",
        "\n",
        "os.chdir(\"C:\\\\Users\\\\shubh\\\\OneDrive\\\\Documents\\\\shubhika\\\\annotated_files\")\n",
        "\n",
        "for file in os.listdir('C:\\\\Users\\\\shubh\\\\OneDrive\\\\Documents\\\\shubhika\\\\annotated_files'):\n",
        "    #print(file)\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        shubhika_list_df.append(df)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "shubhika_df = pd.concat(shubhika_list_df)\n",
        "print(shubhika_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed7cf87b",
      "metadata": {
        "id": "ed7cf87b"
      },
      "outputs": [],
      "source": [
        "shubhika_df.head()\n",
        "shubhika_df=shubhika_df.drop(\"transcript\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a05a510",
      "metadata": {
        "id": "8a05a510",
        "outputId": "e41c4a5c-6017-4261-95f8-000ad1d14aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3130, 3)\n"
          ]
        }
      ],
      "source": [
        "shubhika_df = shubhika_df.dropna()\n",
        "\n",
        "#shubhika_df_1 = shubhika_df.iloc[:10 ,]\n",
        "print(shubhika_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36318ca9",
      "metadata": {
        "id": "36318ca9",
        "outputId": "e294af38-6348-4460-99f1-6e9b1a55ffe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2701, 2)\n"
          ]
        }
      ],
      "source": [
        "shubhika_df = shubhika_df.drop(['id'], axis = 1)\n",
        "shubhika_df = shubhika_df.drop_duplicates()\n",
        "\n",
        "#shubhika_df_1 = shubhika_df.iloc[:10 ,]\n",
        "print(shubhika_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bce704a",
      "metadata": {
        "id": "2bce704a"
      },
      "outputs": [],
      "source": [
        "Pre_war_background_to_the_incarceration = []\n",
        "Governments_decision_to_remove_ethnic_Japanese = []\n",
        "Life_after_removal_and_during_the_incarceration = []\n",
        "Military_services = []\n",
        "Returning_of_Japanese_American_after_WWII = []\n",
        "Legal_challenges = []\n",
        "Redress_movement = []\n",
        "\n",
        "\n",
        "for i in range(shubhika_df.shape[0]):\n",
        "    \n",
        "    if \"Pre-war background to the incarceration\" in shubhika_df.iloc[i, 1]:\n",
        "        #shubhika_df.loc[i, \"Pre_war_background_to_the_incarceration\"] = 1\n",
        "        Pre_war_background_to_the_incarceration.append(1)\n",
        "    else:\n",
        "        Pre_war_background_to_the_incarceration.append(0)\n",
        "    \n",
        "    if \"Government’s decision to remove ethnic Japanese\" in shubhika_df.iloc[i, 1]:\n",
        "        Governments_decision_to_remove_ethnic_Japanese.append(1)\n",
        "    else:\n",
        "        Governments_decision_to_remove_ethnic_Japanese.append(0)        \n",
        "        \n",
        "    if \"Life after removal and during the incarceration\" in shubhika_df.iloc[i, 1]:\n",
        "        Life_after_removal_and_during_the_incarceration.append(1)\n",
        "    else:\n",
        "        Life_after_removal_and_during_the_incarceration.append(0)\n",
        "        \n",
        "    if \"Military services\" in shubhika_df.iloc[i, 1]:\n",
        "        Military_services.append(1)\n",
        "    else:\n",
        "        Military_services.append(0)\n",
        "        \n",
        "    if \"Returning of Japanese American after WWII\" in shubhika_df.iloc[i, 1]:\n",
        "        Returning_of_Japanese_American_after_WWII.append(1)\n",
        "    else:\n",
        "        Returning_of_Japanese_American_after_WWII.append(0)\n",
        "        \n",
        "    if \"Legal challenges\" in shubhika_df.iloc[i, 1]:\n",
        "        Legal_challenges.append(1)\n",
        "    else:\n",
        "        Legal_challenges.append(0)\n",
        "        \n",
        "    if \"Redress movement\" in shubhika_df.iloc[i, 1]:\n",
        "        Redress_movement.append(1)\n",
        "    else:\n",
        "        Redress_movement.append(0)\n",
        "    \n",
        "    \n",
        "    \n",
        "shubhika_df['Pre_war_background_to_the_incarceration'] = Pre_war_background_to_the_incarceration\n",
        "shubhika_df[\"Government’s_decision_to_remove_ethnic_Japanese\"] = Governments_decision_to_remove_ethnic_Japanese\n",
        "shubhika_df['Life_after_removal_and_during_the_incarceration'] = Life_after_removal_and_during_the_incarceration\n",
        "shubhika_df['Military_services'] = Military_services\n",
        "shubhika_df['Returning_of_Japanese_American_after_WWII'] = Returning_of_Japanese_American_after_WWII\n",
        "shubhika_df['Legal_challenges'] = Legal_challenges\n",
        "shubhika_df['Redress_movement'] = Redress_movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f99408",
      "metadata": {
        "id": "03f99408",
        "outputId": "34499386-d70b-4ee6-b46e-866825435732"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6905ec2",
      "metadata": {
        "id": "d6905ec2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "cleaned_text = []\n",
        "for i in shubhika_df['text']:\n",
        "    review = re.sub('[^a-zA-Z]', ' ', i)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if (not word in set(stopwords) and len(lemmatizer.lemmatize(word)) > 1)]\n",
        "    review = ' '.join(review)\n",
        "    cleaned_text.append(review)\n",
        "    \n",
        "shubhika_df['Cleaned_text'] = cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0df825",
      "metadata": {
        "id": "db0df825"
      },
      "outputs": [],
      "source": [
        "x = shubhika_df['Cleaned_text']\n",
        "y = shubhika_df.drop(['text', 'label', 'Cleaned_text'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ee02dd",
      "metadata": {
        "id": "55ee02dd"
      },
      "outputs": [],
      "source": [
        "tf_idf = TfidfVectorizer(ngram_range=(1, 3))\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=42, test_size=0.25)\n",
        "\n",
        "X_train_tf = tf_idf.fit_transform(xtrain)\n",
        "X_test_tf = tf_idf.transform(xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd09e3ba",
      "metadata": {
        "id": "cd09e3ba"
      },
      "source": [
        "# BinaryRelevance model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8869c879",
      "metadata": {
        "id": "8869c879",
        "outputId": "6352382f-d96b-41e0-e319-bf5de972e9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in c:\\users\\shubh\\anaconda3\\lib\\site-packages (0.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-multilearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a3095e",
      "metadata": {
        "id": "87a3095e",
        "outputId": "2ae31280-0cf0-462e-eeae-2559bf34b97c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.5798816568047337\n",
            "F1 score =  0.565643370346179\n",
            "Hamming loss =  0.14053254437869822\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "br_classifier = BinaryRelevance(classifier = gnb)\n",
        "\n",
        "br_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = br_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568ed88b",
      "metadata": {
        "id": "568ed88b"
      },
      "source": [
        "# ClassifierChain model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b78746",
      "metadata": {
        "id": "06b78746",
        "outputId": "25d49f1c-8109-49f0-a219-2714f2e6274a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.5798816568047337\n",
            "F1 score =  0.6202812731310141\n",
            "Hamming loss =  0.10841081994928149\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "chain_classifier = ClassifierChain(classifier = gnb)\n",
        "\n",
        "chain_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = chain_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffafe684",
      "metadata": {
        "id": "ffafe684"
      },
      "source": [
        "# Label PowerSet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305c64f7",
      "metadata": {
        "id": "305c64f7",
        "outputId": "3c8557d5-6fdd-47d6-bb3a-88285075c858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.584319526627219\n",
            "F1 score =  0.6059291395516992\n",
            "Hamming loss =  0.11517328825021132\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "labpower_classifier = LabelPowerset(DecisionTreeClassifier(max_depth = 200))\n",
        "\n",
        "labpower_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = labpower_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fbd784",
      "metadata": {
        "id": "80fbd784",
        "outputId": "61968cd9-19f8-4148-c184-85df2865c3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.5710059171597633\n",
            "F1 score =  0.5938615274803712\n",
            "Hamming loss =  0.1202451394759087\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "labpower_classifier = LabelPowerset(DecisionTreeClassifier(max_depth = 200, criterion='entropy'))\n",
        "\n",
        "labpower_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = labpower_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434e4213",
      "metadata": {
        "id": "434e4213",
        "outputId": "7ff9b9d2-8376-4679-94e5-9b4a4b91fb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.6050295857988166\n",
            "F1 score =  0.631578947368421\n",
            "Hamming loss =  0.11094674556213018\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "labpower_classifier = LabelPowerset(GaussianNB())\n",
        "\n",
        "labpower_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = labpower_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b585045",
      "metadata": {
        "id": "7b585045"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce04474",
      "metadata": {
        "id": "fce04474"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}