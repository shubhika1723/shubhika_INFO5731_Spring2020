{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhika1723/shubhika_INFO5731_Spring2020/blob/main/In_class_exercise_02_09222022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-VkpmGS7TaP"
      },
      "source": [
        "## The second In-class-exercise (09/22/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxGp1SW7TaS"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR_yOu1t7TaT"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnUI7nZt7TaT"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Domain problem: \n",
        ". My specific challenge is to analyze the text information gathered from cricket interviews.\n",
        "\n",
        "Research question:\n",
        ". Create a topic model using the subjects that the cricket player discussed in the interview.\n",
        ". Create the interview data.\n",
        "\n",
        "Information needed to address the issues:\n",
        "The characteristics of the data needed to respond to the research question are as follows. \n",
        ". Interview questions and responses are included in the transcript.\n",
        "\n",
        "\n",
        "We can gather 1042 interview samples from the following link: https://www.cricketinterviews.com/\n",
        "\n",
        "\n",
        "Steps for collecting and saving the data:\n",
        ". We use web scraping technique to scrape the data.\n",
        ". We use BeautifulSoup library to parse the Densho Digital Repository.\n",
        ". We parse through each page and go to the link and then to interviews hyperlink.\n",
        ". We get the info in homepage.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlxTCUiB7TaU"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AHD8tmOS7TaU"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import re\n",
        "\n",
        "total_count = 0\n",
        "count = 0\n",
        "interview_count = 0\n",
        "# url of the densho digital repository to scrape the data\n",
        "url = \"https://www.cricketinterviews.com/\"\n",
        "for page_num in range(1, 38):\n",
        "    # we create the data soup of the main url page html elements.\n",
        "    link1 = Request(url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    #url1 = urlopen(link1)\n",
        "\n",
        "    data1 = url1.read()\n",
        "    data1_soup = BeautifulSoup(data1)\n",
        "    \n",
        "    #print(\"*** Page {} ***\".format(page_num))\n",
        "    # we iterate through each narrator page link\n",
        "    for narrator_link in data1_soup.find_all('h4'):\n",
        "        link2 = Request(narrator_link.a.get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        url2 = urlopen(link2)\n",
        "\n",
        "                        \n",
        "                    \n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac8JiPR_7TaV"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# url of the densho digital repository to scrape the data\n",
        "url = \"https://scholar.google.com/scholar?start={}&q=information+retrieval&hl=en&as_sdt=0,44\"\n",
        "df = pd.DataFrame()\n",
        "dict = {\"Titles\": [], \"Author_and_Year\": [], \"Abstract\": []}\n",
        "for num in range(1, 20):\n",
        "    # we create the data soup of the main url page html elements.\n",
        "    num=num + 9\n",
        "    link1 = Request(url.format(num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    url1 = urlopen(link1)\n",
        "    data1 = url1.read()\n",
        "    data1_soup = BeautifulSoup(data1)\n",
        "    \n",
        "    title_list = data1_soup.find_all(\"h3\", {\"class\": \"gs_rt\"})\n",
        "    author_year_list = data1_soup.find_all(\"div\", {\"class\": \"gs_a\"})\n",
        "    abstract_list = data1_soup.find_all(\"div\", {\"class\": \"gs_rs\"})\n",
        "    \n",
        "    for title in title_list:\n",
        "        dict['Titles'].append(title.text)\n",
        "    for author_year in author_year_list:\n",
        "        dict['Author_and_Year'].append(author_year.text)\n",
        "    for abstract in abstract_list:\n",
        "        dict['Abstract'].append(abstract.text)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(dict)    \n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWi59-DWF7xz",
        "outputId": "4ae6fe1c-ca98-4d02-91d4-b442a07c02e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Titles  \\\n",
            "0          [BOOK][B] Information retrieval interaction   \n",
            "1    [PDF][PDF] Evaluation of evaluation in informa...   \n",
            "2                                Information retrieval   \n",
            "3           [CITATION][C] Modern information retrieval   \n",
            "4    A definition of relevance for information retr...   \n",
            "..                                                 ...   \n",
            "185  Adarank: a boosting algorithm for information ...   \n",
            "186    [HTML][HTML] Geographical information retrieval   \n",
            "187             Terrier information retrieval platform   \n",
            "188  [BOOK][B] Information retrieval: data structur...   \n",
            "189                  Distributed information retrieval   \n",
            "\n",
            "                                       Author_and_Year  \\\n",
            "0                 P Ingwersen - 1992 - oarklibrary.com   \n",
            "1    T Saracevic - … on Research and development in...   \n",
            "2            Q Mei, DR Radev - 2014 - academic.oup.com   \n",
            "3          BY Ricardo - 1999 - Pearson Education India   \n",
            "4    WS Cooper - Information storage and retrieval,...   \n",
            "..                                                 ...   \n",
            "185  J Xu, H Li - … on Research and development in ...   \n",
            "186  CB Jones, RS Purves - … Journal of Geographica...   \n",
            "187  I Ounis, G Amati, V Plachouras, B He… - … on I...   \n",
            "188       WB Frakes, R Baeza-Yates - 1992 - dl.acm.org   \n",
            "189  J Callan - Advances in information retrieval, ...   \n",
            "\n",
            "                                              Abstract  \n",
            "0    … Information retrieval covers the problems re...  \n",
            "1    ABSTRACT Evaluation is a major force in resear...  \n",
            "2    Information Retrieval | The Oxford Handbook of...  \n",
            "3                                                       \n",
            "4    … The purpose of this paper, then, is to propo...  \n",
            "..                                                 ...  \n",
            "185  In this paper we address the issue of learning...  \n",
            "186  … geographic relevance and the non‐retrieval o...  \n",
            "187  Terrier is a modular platform for the rapid de...  \n",
            "188  … covering the major information retrieval alg...  \n",
            "189  A multi-database model of distributed informat...  \n",
            "\n",
            "[190 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xUvg8k47TaW"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eAKxA9ja7TaW"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "####input your credentials here\n",
        "consumer_key = 'u7L11nR7HN85dn1qnTFO1cegb'\n",
        "consumer_secret = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'\n",
        "access_token = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07co'\n",
        "access_token_secret = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRs1Jce'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "#####United Airlines\n",
        "# Open/Create a file to append data\n",
        "csvFile = open('ua.csv', 'a')\n",
        "#Use csv Writer\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}